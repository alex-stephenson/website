---
title: "Final_dissertation_script"
output:
  word_document: default
  html_document: default
always_allow_html: yes
---

Below is the programming script for the paper _Towards a theory of Russian information manipulation: What do tweets from the ‘Internet Research Agency’ tell us about Russia’s misinformation strategy_, using the programming language 'R'. The code roughly follows the order of analysis in the paper, and annotations have been added to assist the reader in understanding the code. 

Initially, I set the parameters for the paper 

```{r setup, include=FALSE}
options(digits = 2, scipen = 999)
set.seed(1234)
```


Now time to add the libraries 

```{r message=F, warning=F, echo=F}
library(gt) #for making tables
library(tidyverse) 
library(lubridate) #dates
library(tidytext) #text analysis
library(stm) #stm
library(ggthemes) #for ggplot2 themes
library(patchwork) #for patching together plots 
library(tidystm) #STM eval but tidy
library(igraph) #network analysis
library(ggraph) #network analysis 
library(tidygraph) #more network analysis but tidy so can wrangle graphs
library(SnowballC) #stemming words
library(furrr) #in order to implement tidy parallel processing
plan(multiprocess) #engage multiple cores

```


Data import and clean

```{r message=F, warning=F, error=FALSE}
data <- read_csv("ira_tweets_csv_hashed.csv")

tweets <- data %>% 
  filter(tweet_language == "ru" | account_language == "ru") #filter for only Russian tweets

rm(data)

tweets <- tweets %>% 
  select(-user_display_name, -user_profile_description, -user_profile_url, -user_reported_location, -account_creation_date, -longitude, -latitude, -poll_choices) #remove variables not used


user_data_info <- read_csv("/Users/alexstephenson/Desktop/R_Studio_Work/Dissertation/Dissertation_R/ira_users_csv_hashed.csv") #data about accounts, not tweets

user_data_info <- user_data_info %>% 
  filter(account_language == "ru")



### import stopwords lexicon

english_stopwords <- get_stopwords("en")
stopwords_ru <- read_delim("/Users/alexstephenson/Desktop/R_Studio_Work/Dissertation/Dissertation_R/stopwords-ru.txt", delim = ",", col_names  = F)
names(stopwords_ru) <- c('word')
stopwords_ru <- as_tibble(stopwords_ru)

custom_stop_words <- bind_rows(tibble(word = c("https", "t.co", "rt", "amp", "все", "это", 'мной', 'почему', 'спасибо', 'просто', 'очень', "привет", "всё", "тебе", "вообще", "день", "думаю", "пока", "такое", "такие", "таких", "изза", "ещё", "знаю")),  
                               stopwords_ru)
extra_stop_words <- c('сам', 'хочет', 'наш')
```




#### Competency analysis

These are the competency dictionaries provided by Guriev and Treisman (2019b)

```{r}
violence_dictionary<-c('мертвый','смерть','смертельный','случайный','умереть','умер','умирает','умирать','уничтожить','уничтожить','смертельный','похороны','холокост','убить','бойня','скорбеть','убить','война','войны','воюя','разбить','разгромить','маршруты','маршрутизация','забастовка','ударил','беспокоящий','конфликт','враждебное','оружие','пистолет','оружие','застрелен','битва','сражения','вооруженные','больно','ранит','вред','вредугнетать','уничтожать','разрушать','тюрьма','наказывать','порабощать','раб','добыча','кровь','кровоточить','кровоточить','мученик','мученики','мученики','армии','армия','боль','болезненные','боли','вторжение','вторжение','насилие','насильственный','взорваться','взрывается','бомба','раздавить','ранить','ранить','сражаться','преследовать','тиранизировать','уничтожить','перестрелка','солдат','завоевать','пушка','террор','терроризм','террорист','злодеяние','зверства','жестокие','жестокие','мучения','штык','старв','осада','сдача','разбить','вооружение','танки','артиллерия','миномет','броня','завоевание','военный','крестовыйпоход','преступник','преступление','арест','просекут','флот','враг','враги','вражда','пленник','бич','мутилат','гибель','опустошение','варвар','полиция','побеждение','жертва','заложник','пуля','оружие','мясник','гибель','войска','грабеж','ненависть','страдания','бригада','задержание','ликвидация','жестокоеобращение','тюремноезаключение','заключениеподстражу','заложники')

```

```{r}
economic_dictionary<-c('доступный','аудитор','аудиторы','одолжить','купил','бюджет','купить','дешево','дешевле','валюта','клиент','долг','депозит','скидка','доллар','доллары','заработок','эконом','рецессия','аренда','розничнаяторговля','выручка','богаче','богатство','богатейший','салар','продажа','продажа','экономия','продажа','продажа','магазин','продажа','магазин','торговля','торговля','заработнаяплата','заработнаяплата','богатство','богаче','богатейших','богатых','обмен','расходы','дорого','финансы','фонд','доход','страхование','инвестирование','инвестиции','инвестирование','инвестирование','аренда','кредитование','кредитование','кредит','рынок','купец','деньги','монополия','ипотека','пенсия','песеты','бедность','цена','цены','прибыль','покупки','зарплата','акции','коммерция','рост','работа','работа','продукция','промышленность','отрасли','промышленность','индустриализация','индустриализация','производство','труд','труд','труд','труд','труд','работа','продукция','потребитель','фабрика','фабрики','remunerat','товары','занятые','безработица','инфляция','сельскоехозяйство','аграрныйсектор','тариф','рацион','нормирование','экспорт','импорт','импорт','импорт','выпуск','предприниматель','эффективность','проспэр','дефицит','сельскоехозяйство','выращивание')
```


```{r}
social_dictionary<-c('расходы','медицинские','медицина','образование','жилье','школа','школы','университеты','университет','класснаякомната','уходзадетьми','больница','больницы','доктор','материнство','инфраструктура','грамотность','администрация','транспорт','выходнапенсию','финансирование','инвалид','доход','бюджет','сборы','фонд','страхование','пенсия')
```


```{r}
competency_dictionary <- c(violence_dictionary, economic_dictionary, social_dictionary)
competency_dictionary_or <- paste(competency_dictionary, collapse = "|") #add logical operators for analysis

violence_dictionary_or <- paste(violence_dictionary, collapse = "|")
economic_dictionary_or <- paste(economic_dictionary, collapse = "|")
social_dictionary_or <- paste(social_dictionary, collapse = "|")
```


Competency tokens

```{r}
competency_tokens <- tweets %>%  
  select(tweet_text, tweet_time) %>% 
  mutate(tweet_text = tolower(tweet_text),
         tweet_time = as.Date(tweet_time),
         tweet_time = floor_date(tweet_time, "2 weeks")) %>% #tweets rounded to two week periods
  unnest_tokens(tokens, tweet_text, token = "words") #unnested so one row per word
  

competency_tokens_filtered <- competency_tokens %>% 
  add_count(tweet_time, name = 'obs') %>% 
  filter(tokens %in% c(economic_dictionary, violence_dictionary, social_dictionary)) #keep only competency words
  
competency_tokens_filtered$theme[competency_tokens_filtered$tokens %in% economic_dictionary] <- 'Economic'
competency_tokens_filtered$theme[competency_tokens_filtered$tokens %in% violence_dictionary] <- 'Violent'
competency_tokens_filtered$theme[competency_tokens_filtered$tokens %in% social_dictionary] <- 'Social'


competency_tokens_processed <- competency_tokens_filtered %>% 
  mutate(theme = as.factor(theme)) %>% 
  add_count(tweet_time, theme, name = 'n') %>% 
  transmute(tweet_time, theme, percent = (n / obs * 100)) %>% 
  filter(tweet_time > as.Date("2012-01-01") & tweet_time < as.Date("2017-06-01")) %>% 
  distinct() #calculate frequency of tokens


competency_tokens_graph <- competency_tokens_processed %>% 
  ggplot(aes(tweet_time, percent, col = theme)) +
  geom_line(alpha = 0.5) +
  theme_solarized() +
  scale_color_solarized() +
  scale_x_date(breaks = scales::date_breaks(width = "1 year"),
                   labels = scales::date_format(format = "%Y"),
                   minor_breaks = "6 months") +
  expand_limits(y = c(0, 1.5)) +
  labs(title = "Frequency of 'competency' words as a % of all words",  x = "", y = 'Frequency of terms (%) in IRA tweets', color = "Competency")  +
  theme(legend.position = "none")

competency_tokens_graph <- competency_tokens_graph + 
  theme(axis.title.y = element_text(size = 20),
        plot.title = element_text(size = 25),
        axis.text.x = element_text(size = 16),
        axis.text.y = element_text(size = 16)) #graph formatting

```

The next code chunk calculates not the percentage frequency of tokens but the frequency of tweets mentioning one competency word

```{r}
competency_tweets <- tweets %>%  
  select(tweet_text, tweet_time) %>% 
  mutate(tweet_text = tolower(tweet_text),
         tweet_time = as.Date(tweet_time),
         tweet_time = floor_date(tweet_time, "2 weeks"))

violence_tweets_filtered <- competency_tweets %>% 
  add_count(tweet_time, name = 'obs') %>% 
  filter(str_detect(tweet_text, violence_dictionary_or)) %>% 
  mutate(theme = "Violent")
  
economic_tweets_filtered <- competency_tweets %>% 
  add_count(tweet_time, name = 'obs') %>% 
  filter(str_detect(tweet_text, economic_dictionary_or)) %>% 
  mutate(theme = "Economic")

social_tweets_filtered <- competency_tweets %>% 
  add_count(tweet_time, name = 'obs') %>% 
  filter(str_detect(tweet_text, social_dictionary_or)) %>% 
  mutate(theme = "Social")

competency_tweets_filtered <- rbind(violence_tweets_filtered, economic_tweets_filtered, social_tweets_filtered)

competency_tweets_processed <- competency_tweets_filtered %>% 
  filter(tweet_time > as.Date("2012-01-01") & tweet_time < as.Date("2017-06-01")) %>% 
  mutate(theme = as.factor(theme)) %>% 
  add_count(tweet_time, theme, name = 'n') %>% 
  transmute(tweet_time, theme, percent = (n / obs * 100)) %>% 
  #filter(percent < 3) %>% #removing the last week in 2009 where there were only a few tweets
  distinct()

competency_tweets_graph <- competency_tweets_processed %>% 
  ggplot(aes(tweet_time, percent, col = theme)) +
  geom_line(alpha = 0.5) +
  #scale_color_brewer(palette = "Paired", direction = -2) +
  theme_solarized() +
  scale_color_solarized() +
  scale_x_date(breaks = scales::date_breaks(width = "1 year"),
                   labels = scales::date_format(format = "%Y")) +
                   #minor_breaks = "6 months") +
  #expand_limits(y = c(0, 2)) +
  labs(title = "IRA tweets mentioning at least one 'Competency' key words",  x = "", y = "Frequency of terms (%)", color = "Competency")


competency_tweets_graph <- competency_tweets_graph + 
  theme(axis.title.y = element_text(size = 22),
        axis.text.x = element_text(size = 16),
        axis.text.y = element_text(size = 16),
        plot.title = element_text(size = 25))

```


Graph objects

```{r fig.height=6}
patchwork <- competency_tokens_graph / competency_tweets_graph + plot_layout(guides = "collect")
patchwork +
  plot_annotation(
    title = "Competency Topic Frequencies",
    theme = theme_solarized(base_size = 30)
  ) +
  theme(legend.text = element_text(size = 20),
        legend.title = element_text(size = 26),
      legend.key = element_rect(ggthemes_data$solarized$Base$name['base3']),
      legend.key.size = unit(4, "lines"),
      axis.title.y = element_text(size = 20)) #formatting
      
      


```


This code chunk analyses the metadata for tweets mentioning competency topics

```{r}
economic_token_freq <- competency_tokens_filtered %>% 
  filter(theme == 'Economic') %>% 
  summarise(token_pcnt = nrow(.) / nrow(competency_tokens) * 100) #shoudnt this be competency_tweets?

social_token_freq <- competency_tokens_filtered %>% 
  filter(theme == 'Social') %>% 
  summarise(token_pcnt = nrow(.) / nrow(competency_tokens) * 100)

violent_token_freq <- competency_tokens_filtered %>% 
  filter(theme == 'Violent') %>% 
  summarise(token_pcnt = nrow(.) / nrow(competency_tokens) * 100)

#violent tweets

violent_tweets_info <- tweets %>%
  mutate(rows = nrow(.)) %>% 
  mutate(tweet_text = tolower(tweet_text)) %>%  
  filter(str_detect(tweet_text, paste(violence_dictionary, collapse = '|'))) %>%
  mutate(pcnt_total = nrow(.) / rows * 100)

violent_tweets_summary <- violent_tweets_info %>% 
  summarise(Topic = 'Violent Tweets',
             `% of tweets using competency words` = economic_token_freq$token_pcnt,
              `Competence words as a % of all words` = mean(pcnt_total),
            `Mean replies per Tweet` = mean(reply_count, na.rm = T),
            `Max Reply` = max(reply_count, na.rm = T),
            `Mean Retweets per Tweet` = mean(retweet_count, na.rm = T),
            `Max Retweets` = max(retweet_count, na.rm = T))


#social tweets

social_tweets_info <- tweets %>%
  mutate(rows = nrow(.)) %>% 
  mutate(tweet_text = tolower(tweet_text)) %>%  
  filter(str_detect(tweet_text, paste(social_dictionary, collapse = '|'))) %>%
  mutate(pcnt_total = nrow(.) / rows * 100) 

social_tweets_summary <- social_tweets_info %>% 
    summarise(Topic = 'Social Tweets',
              `% of tweets using competency words` = economic_token_freq$token_pcnt,
              `Competence words as a % of all words` = mean(pcnt_total),
            `Mean replies per Tweet` = mean(reply_count, na.rm = T),
            `Max Reply` = max(reply_count, na.rm = T),
            `Mean Retweets per Tweet` = mean(retweet_count, na.rm = T),
            `Max Retweets` = max(retweet_count, na.rm = T))


#economic tweets

economic_tweets_info <- tweets%>%
    mutate(rows = nrow(.)) %>% 
    mutate(tweet_text = tolower(tweet_text)) %>%  
    filter(str_detect(tweet_text, paste(economic_dictionary, collapse = '|'))) %>%
    mutate(pcnt_total = nrow(.) / rows * 100)

economic_tweets_summary <- economic_tweets_info %>% 
    summarise(Topic = 'Economic Tweets', 
              `% of tweets using competency words` = economic_token_freq$token_pcnt,
             `Competence words as a % of all words` = mean(pcnt_total),
            `Mean replies per Tweet` = mean(reply_count, na.rm = T),
            `Max Reply` = max(reply_count, na.rm = T),
            `Mean Retweets per Tweet` = mean(retweet_count, na.rm = T),
            `Max Retweets` = max(retweet_count, na.rm = T))


gt_competency_summaries <- rbind(economic_tweets_summary, social_tweets_summary, violent_tweets_summary) %>% 
  gt(rowname_col = "Topic") %>% 
  tab_header(
    title = "Analysis of Competency Tweets",
    subtitle = "Comparing metadata statistics between the 'competency topics' of Informational Autocracy"
  )  %>% 
  tab_footnote(
    footnote = md('_Guriev and Treisman (2019a, 2019b)_'),
    locations = cells_title(groups = c("subtitle"))
  )

```



#### Meta data analysis


```{r fig.width=10}

tweet_dates <- tweets %>%
  mutate(year = lubridate::year(tweet_time)) %>%
  filter(year != 2009) %>% 
  mutate(tweet_time = lubridate::week(tweet_time)) %>% 
  select(year, tweet_time)

tweet_dates %>% 
  add_count(year, tweet_time) %>% 
  ggplot(aes(tweet_time, n)) +
  geom_line() +
  facet_wrap(~year, scales = "free_y", ncol = 3)

tweet_dates_graph <- tweet_dates %>% 
  add_count(year, tweet_time) %>% 
  ggplot(aes(tweet_time, n)) +
  geom_line() +
  facet_wrap(~year, scales = "free_y", ncol =3) +
  theme_solarized() +
  scale_x_continuous(breaks = seq(0, 52, 4),
                     labels = c(seq(0, 52, 4))) +
  labs(title = "Time series analysis of 5.4m known IRA tweets by year", x = "Week", y = "") +
  theme(plot.title = element_text(hjust = 0, size = 25, color = "black")) +
  theme_solarized() +
  theme(axis.text.x = element_text(color = "grey20", size = 14, angle = 0, hjust = .5, vjust = .5, face = "plain"),
        axis.text.y = element_text(color = "grey20", size = 14, angle = 0, hjust = 1, vjust = 0, face = "plain"),  
        axis.title.x = element_text(color = "grey20", size = 25, angle = 0, hjust = .5, vjust = 0, face = "plain"),
        axis.title.y = element_text(color = "grey20", size = 12, angle = 90, hjust = .5, vjust = .5, face = "plain")) +
  theme(plot.title = element_text(size=30, face = "plain", colour = "black"))

tweet_dates_graph

```








```{r livejournal_analysis}
urls <- tweets %>% 
  select(urls) %>% 
  filter(!is.na(urls)) %>% 
  filter(urls != '[]') 


unique_links <- urls %>% 
  count(urls)
  




shortened_urls <- urls %>% 
  filter(!str_detect(urls, c('bit.ly'))) %>% #remove the non-intelligble links
  filter(!str_detect(urls, c('goo.gl'))) %>% 
  filter(!str_detect(urls, c('j.mp'))) %>% 
  filter(!str_detect(urls, c('dlvr.it'))) %>% 
  filter(!str_detect(urls, c('tinyurl')))


news_sources <- c('news', 'new', 'ria', 'bbc', 'nyt', 'cnn', 'tass', 'vesti', 'gezeta', 'rbc', 'rt', 'meduzaproject', 'LIFENEWS_RU', 'harkovnews', 'Pravdiva_pravda')
news_sources <- paste(news_sources, collapse = "|")


live_journal_urls <- urls %>% 
  filter(str_detect(urls, 'livejournal')) %>% #some regex to isolate livejournal urls
  mutate(urls = str_replace_all(urls, 'http://', '')) %>% 
  mutate(urls = str_replace_all(urls, '[0-9]+\\.html', '')) %>% 
  mutate(urls = str_replace_all(urls, 'livejournal', '')) %>% 
  mutate(urls = str_replace_all(urls, '.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?$', '')) %>% 
  mutate(urls = str_replace_all(urls, 'co', '')) %>% 
  mutate(urls = str_replace_all(urls, 'com', '')) 

unique_live_journal_urls <- live_journal_urls %>% 
  count(urls) %>% 
  nrow()

news_tweets <- urls %>% 
  filter(str_detect(urls, news_sources)) %>% 
  nrow()

cat('Number of urls =', nrow(urls), '\nNumber of unique links =', nrow(unique_links), '\nNumber of links to LiveJournal posts =', nrow(live_journal_urls), '\nNumber of unique LiveJournal links =', unique_live_journal_urls, '\nNumber of tweets mentioning news organisations (conservative estimate) =', news_tweets) #maybe look into using {glue} for this


```





#### Network analysis

```{r}

retweet_network_data <- tweets %>% 
  filter(retweet_userid %in% tweets$userid) %>% #filter for internal retweets
  select(userid, retweet_userid) %>% 
  na.omit() %>% 
  add_count(retweet_userid, userid, sort = T) %>%  #count the number of times accounts were retweeted
  distinct(.) #remove duplicates
  
retweet_network_data <- retweet_network_data %>% 
  rename(weight = n)

 
retweet_network_graph <- retweet_network_data %>% 
  filter(weight>50) %>% #remove from visualisation links that have a weight less than 50
  graph_from_data_frame() #this is a bit of a hack to turn the wrangled data frame back into a graph object

retweet_network_graph_df <- igraph::as_data_frame(retweet_network_graph, what = "both")
retweet_nodes <- retweet_network_graph_df$vertices %>% 
  mutate(degree = igraph::degree(retweet_network_graph),
         strength = igraph::strength(retweet_network_graph)) #make a separate object that contains details about nodes so I can include 


tidy_network <- as_tbl_graph(retweet_network_graph) 

tidy_network %>% 
  activate(nodes) %>% 
  mutate(strength = strength(tidy_network)) %>% 
  activate(edges) %>% 
  mutate(weight > 20) %>% 
  ggraph(layout = "stress") +
  geom_edge_link(aes(alpha = weight)) +
  geom_node_point(color = ifelse(strength(retweet_network_graph) > 1000, 'red', 'blue')) +
  theme_graph() +
  labs(title = "Network of IRA accounts", caption = "Red dots are accounts with a strength greater than 10000\nLine opacity represents the weight of the relationship")    
  


```


Was the relationship between the small number of influential accounts and the supporting accounts reciprocal?






```{r}
top_17_rt_strength <- retweet_nodes %>% 
  arrange(desc(strength)) %>%  
  top_n(strength, n = 17)


non_influential_accounts <- user_data_info %>% 
  filter(!userid %in% top_17_rt_strength$userid)

tweets %>% 
  filter(userid %in% top_17_rt_strength$name) %>% 
  select(userid, in_reply_to_userid) %>% 
  na.omit() %>% 
  filter(in_reply_to_userid %in% non_influential_accounts$userid) %>% #at this stage there are only six accounts that engage internally
  add_count(userid) %>% 
  dplyr::distinct(n, .keep_all = T) %>% 
  select(-in_reply_to_userid) %>% 
  rename(`Times tweeting non-influential accounts` = n)
 

```






```{r}

number_of_replies <- tweets %>% 
  filter(!in_reply_to_userid %in% user_data_info$userid) %>% 
  select(in_reply_to_tweetid) %>% 
  na.omit() %>% 
  nrow()


number_of_retweets <- tweets %>% 
  #filter(!retweet_userid %in% user_data_info$userid) %>% 
  select(retweet_userid) %>% 
  na.omit() %>% 
  nrow()


tweets_meta_data <- tweets %>% 
  summarise( 
    Account = 'All Accounts',
    `Mean number of replies` = mean(reply_count, na.rm =T),
    `Mean number of retweets` = mean(retweet_count, na.rm = T),
    `Was a reply (%)` = number_of_replies / nrow(tweets) * 100,
    `Was a retweet (%)` = number_of_retweets / nrow(tweets) * 100
  )





### Lets do the same process but for influential accoun


number_of_influential_replies <- tweets %>% 
  filter(userid %in% top_17_rt_strength$name) %>% 
  select(in_reply_to_tweetid) %>% 
  na.omit() %>% 
  nrow()


number_of_influential_retweets <- tweets %>% 
  filter(userid %in% top_17_rt_strength$name) %>% 
  select(retweet_userid) %>% 
  na.omit() %>% 
  nrow()

tweets_influential_accounts <- tweets %>% 
  filter(userid %in% top_17_rt_strength$name)


tweets_influential_accounts_summary <- tweets_influential_accounts %>% 
  summarise( 
    Account = 'Influential Accounts',
    `Mean number of replies` = mean(reply_count, na.rm =T),
    `Mean number of retweets` = mean(retweet_count, na.rm = T),
     `Was a reply (%)` = number_of_influential_replies / nrow(tweets_influential_accounts) * 100,
    `Was a retweet (%)` = number_of_influential_retweets / nrow(tweets_influential_accounts) * 100
  )


gt_meta_data <- rbind(tweets_meta_data, tweets_influential_accounts_summary) %>% 
  gt(rowname_col = 'Account') %>% 
  tab_header(
    title = "Metadata analysis of 5,401,109 IRA tweets",
    subtitle = "Comparing metadata statistics between influential IRA accounts and typical ones"
  ) 

```



Before the topic modelling, here is the that suggests there may have been some change in strategy in the way the IRA engaged with external accounts after July 2014

```{r}

reply_tweets <- tweets %>% 
  filter(str_detect(tweet_text, '^@')) #n = 350k

reply_tweets_filtered <- reply_tweets %>% 
  mutate(rownumber = row_number()) %>% 
  mutate(reply = ifelse(in_reply_to_userid %in% userid, 'Internal', 'External'))


number_of_reply_tweets <- reply_tweets_filtered %>% 
  ggplot(aes(tweet_time)) +
  geom_histogram(bins = 104, show.legend = F) +
  labs(x = "Date", y = "Number of tweets", title = "Number of reply tweets") +
  facet_wrap(~ reply, nrow=2) +
  theme_solarized()


reply_tweets_filtered_2014 <- reply_tweets_filtered %>% 
  filter(tweet_time > as.Date("2014-06-01") & tweet_time < as.Date("2014-08-01")) %>% 
  ggplot(aes(tweet_time)) +
  geom_histogram(bins = 60, show.legend = F) +
  labs(x = "Date", y = "Number of tweets", title = "Number of reply tweets", subtitle = "Between 06/01/2014 -- 01/08/2014") +
  facet_wrap(~ reply, nrow=2) +
  theme_solarized()

number_of_reply_tweets + reply_tweets_filtered_2014


```


#### Structural topic modelling

The data cleaning / manipulation steps here are, at times, a bit convoluted. 

```{r}

pre_mh17_network_data <- tweets %>% 
  filter(tweet_time < as.Date("2014-07-17")) %>%
  filter(retweet_userid %in% tweets$userid) %>% 
  select(userid, retweet_userid) %>% 
  na.omit() %>% 
  add_count(retweet_userid, userid, sort = T) %>% 
  distinct(.) %>% 
  rename(weight = n) %>% 
  graph_from_data_frame()

pre_mh17_network_graph_df <- igraph::as_data_frame(pre_mh17_network_data, what = "both") 

mh17_accounts_nodes <- pre_mh17_network_graph_df$vertices %>% 
  mutate(degree = igraph::degree(pre_mh17_network_data),
         strength = igraph::strength(pre_mh17_network_data))

mh_17_top_100 <- mh17_accounts_nodes %>% 
  arrange(desc(strength)) %>%  
  top_n(strength, n = 50)


influential_MH17_tweets <- tweets %>% 
  filter(tweet_time > as.Date("2014-07-17") & tweet_time < as.Date("2014-08-01")) %>% #filter for tweets in two weeks following crash
  filter(userid %in% mh_17_top_100$name) %>% #filter for influential accounts
  select(tweet_text, userid, tweet_time) %>%
  mutate(rows = row_number())

influential_MH17_tokens <- influential_MH17_tweets  %>% 
  unnest_tokens(word, tweet_text, token = "tweets") %>% #unnest tweets into single word format
  filter(!word %in% stopwords_ru$word) %>% #remove stopwords
  filter(!word %in% english_stopwords$word) %>% 
  filter(!str_detect(word, "^@")) %>% 
  filter(!str_detect(word, "^http")) %>% 
  filter(!word == "rt") %>% 
  mutate(word = SnowballC::wordStem(word, language = "russian")) %>% #stem words
  filter(!word %in% extra_stop_words) #now remove some common stopword stems

influential_MH17_tweets_rebuilt <-  influential_MH17_tokens %>% 
  group_by(rows) %>% 
  summarise(tweet_text = paste(word, collapse = " ")) %>% #now I re-construct the tweets based on the row_number variable
  left_join(influential_MH17_tokens,
              by = 'rows') %>% 
  select(-word) %>% 
  distinct(rows, .keep_all = TRUE) %>% 
  left_join(mh_17_top_100, 
            by = c('userid' = 'name')) #now I have added account details, such as strength


## our dataframe now looks like this:


influential_MH17_tweets_rebuilt %>% head()

```


OK now it's time for some machine learning! The aim of this section  is to better understand what the IRA accounts were talking about. Literature suggests there could be a number of possible outcomes. Firstly, the accounts are not directly talking about political events but 'cheerleading' for the regime. Secondly, the accounts are aiming to distract away from events through creating 'meaninglessness'. Finally, that the accounts are specifically addressing issues in order to frame them a particular way. 

```{r}

processed_tweets <- textProcessor(influential_MH17_tweets_rebuilt$tweet_text, metadata = influential_MH17_tweets_rebuilt)

#plotRemoved(processed_tweets$documents, lower.thresh = seq(1, 100, by = 10)) #this gives an idea of the threshold for removing infrequent numbers 

out <- prepDocuments(processed_tweets$documents, processed_tweets$vocab, processed_tweets$meta) #run out #no lower threshold

docs <- out$documents
vocab <- out$vocab
meta <- out$meta
out$meta$tweet_time <- as.numeric(out$meta$tweet_time - min(out$meta$tweet_time)) #need to convert dates to numeric
```




```{r fig.height=5.5, fig.width=7}


stm_mh17_accounts <- stm(documents = out$documents, vocab= out$vocab, K = 50, prevalence = ~s(tweet_time) + strength, data = out$meta, init.type = "Spectral", seed = 1234, verbose = F, emtol = 1e-4) 

plot(stm_mh17_accounts, type = "summary", xlim = c(0, 0.14), cex = 2)

```

Key words for interesting topics can be seen a bit more clearly here:


```{r fig.height=7, fig.width=9, beta} 
tidy_stm_beta <- tidy(stm_mh17_accounts, matrix = "beta")

tidy_stm_beta_topics <- tidy_stm_beta %>% 
  filter(topic %in% c(5, 25, 38, 43, 49, 20, 42)) %>% 
  group_by(topic) %>% 
  top_n(15, beta) %>% 
  ungroup() %>% 
  arrange(topic, -beta)

beta_12_graph <- tidy_stm_beta_topics %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, color = as.factor(topic))) +
  geom_col(show.legend = FALSE, alpha = 0.75, colour = "dark grey") +
  coord_flip() +
  tidytext::scale_x_reordered() +
  facet_wrap(~topic, scales = "free", ncol = 3) +
  theme_solarized() 

beta_12_graph

```


model viz

```{r}

MH17_sparse <- influential_MH17_tokens %>% 
  count(rows, word) %>% 
  cast_sparse(rows, word, n)


beta_top_terms <- tidy_stm_beta %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(7, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>% 
  unnest(cols = c(terms))

tidy_stm_gamma <- tidy(stm_mh17_accounts, matrix = "gamma", document_names = rownames(MH17_sparse))

gamma_terms <- tidy_stm_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(beta_top_terms, by = "topic") %>% 
  filter(topic %in% c('5', '25', '38', '17', '36', '11', '44', '43', '49', '42')) %>% #remove if not needed - filters out top topics
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))


English_terms <- c('Provation by Kiev, Ukraine, Interesting', 
                    'Kiev tell the truth, Ukraine, Curious',
                   'Kiev shot the boeing, Ukraine, read',
                   'Ukraine, Become, Actually',
                   'Kiev Tell the Truth, Post, Ukranian',
                   'Ukraine, New, Revolution',
                   'No Provocation, ready, provoation',
                   'America, Sanctions, Crimea, Stop Sanctions, Obama',
                   'St Petersburg, Suffering, Death',
                   'Putin, Poroshenko, President')
                   

gt_stm_terms <- data.frame(gamma_terms, English_terms) %>% 
  select(-terms) %>% 
  rename('Terms (in English)' = English_terms) %>% 
  gt(rowname_col = "topic") %>% 
  tab_header(
    title = "Structural Topic Model of IRA tweets about MH17",
    subtitle = "The 10 most prevalent topics with their top 7 terms"
  ) 
```


Estimating topic effect over time


```{r fig.width=5, warning=F, message=F}
prep <- estimateEffect( ~s(tweet_time) + strength, stm_mh17_accounts, meta = out$meta, uncertainty = "Global")


effect <- tidystm::extract.estimateEffect(prep, "tweet_time", method = "continuous") #tidy

#relabel our facet labels
variable_names <- list( 
  "5" = "Topic 5: MH17: #ProvocationByKiev" ,
  "25" = "Topic 25: MH17: #KievTellTheTruth" ,
  "38" = "Topic 38: MH17: #KievShotTheBoeing",
  '43' = 'Topic 43: Sanction, Obama, USA, Crimea',
  '49' = 'Topic 49: Putin, Poroshenko',
  '20' = 'Topic 20: Shelling, Residences, Silovik, Donetsk'
)


#make a function that acts inside the call to labeller in the facet_wrap() function
variable_labeller <- function(variable,value){
  return(variable_names[value])
}


topic_dist_over_time_graph <- effect %>% 
  mutate(covariate.value = as.POSIXct(covariate.value, origin = "2014-07-17 ")) %>% #convert back from numeric into date
  filter(topic %in% c(5, 25, 38, 43, 49, 20)) %>% #select desired topics
  mutate(topic = as.factor(topic)) %>% 
  ggplot(aes(x = covariate.value, y = estimate)) +
  geom_line() +
  facet_wrap(~topic, labeller=variable_labeller, ncol = 2, scales = "free") + #facet and label 
  theme_solarized() +
  theme(legend.position = "none") +
  labs(x = "", title = "Estimated topic distribution over time", subtitle = "Tweets from the 50 most influential IRA accounts for the two week period after 14th July 2014")

    

topic_dist_over_time_graph
```


Lets see the extent to which the MH17 tweets also picked up on the conspiracies being promoted within the media. The topic model did not suggest this was the case, but I want to manually explore whether this is the case. 

```{r}

conspiracy_words <- c('Каратолов','ракета', 'снаряд', 'истребитель', 'спутник')
conspiracy_words <- paste(conspiracy_words, collapse= "|")
tokens <- c('Каратолов', 'бук')
n <- as.numeric(0, 0)
pcnt <- as.numeric(0, 0)


conspiracy_tokens <- tweets %>% 
  select(tweet_time, tweet_text) %>% 
  filter(tweet_time > as.Date("2014-07-17") & tweet_time < as.Date("2014-08-01")) %>% 
  unnest_tokens(tokens, tweet_text, token = "words")

conspiracy_tokens <- conspiracy_tokens %>% 
  mutate(obs = nrow(.))

labels = data.frame(tokens = c("бук", 
                            'истребител', 
                            'Картаполов',
                            "ракет",
                            "снаряд"),
                    len = c('0', '3','0', '72', '49'))



conspiracy_tokens_graph <- conspiracy_tokens %>% 
  filter(str_detect(tokens, conspiracy_words)) %>% 
  mutate(tokens = SnowballC::wordStem(tokens, language = "russian")) %>% 
  dplyr::mutate(tokens = dplyr::recode(tokens, 
                                       'спутников' = 'спутник', 
                                       'биоспутник' = 'спутник')) %>% 
  add_count(tokens) %>% 
  dplyr::distinct(n, .keep_all =T) %>% 
  mutate(pcnt = n / obs * 100) %>% 
  select(-tweet_time, -obs) %>% 
  rbind(data.frame(tokens, n, pcnt)) %>% 
  ggplot(aes(tokens, pcnt)) +
  geom_col() +
  labs(y = "Token frequency as a % of tokens", x = "", title = "Frequency of words associated with conspiracy theories", subtitle = "From tweets between 17th July 2014 and the 1st August 2014") +
  theme_solarized() +
  scale_x_discrete(labels=c("бук" = "Buk", 
                            'истребител' = 'Fighter jet', 
                            'Каратолов'= 'Karatolov',
                            "ракет" = "Rocket",
                            "снаряд" = "Missile",
                            'спутник' = 'Satellite')) +
  scale_y_continuous(sec.axis = sec_axis(~ . *19013.2, name = "Token frequency (n)"))
conspiracy_tokens_graph
```


Are there any influential accounts that aren't included in the IRA list?

```{r fig.height=14}
non_IRA_influential_accounts <- tweets %>% 
  select(userid, retweet_userid) %>% 
  na.omit()  %>% 
  count(retweet_userid, sort = T) %>% 
  dplyr::top_n(100) %>% 
  filter(!retweet_userid %in% tweets$userid) %>% 
  filter(n > 10000) 

non_IRA_influential_accounts$account_name <- c('Rianru', 'GazetaRu', 'RT_russian', 'Vesti', 'GazetaRu_All', 'Pravdiva_pravda', 'LENTA_RU', 'TASS', 'LEPRASORIUM', 'harkovnews', 'LIFENEWS_RU', 'Ru_rbc', 'champ_football', 'izvestia_ru', 'tvrain', 'meduzaproject', 'EchoMskNews', '0cub0')

non_IRA_influential_accounts_table <- non_IRA_influential_accounts %>% 
  select(account_name, n) %>% 
  dplyr::rename('Account name' = account_name,
                'Number of IRA interactions' = n) %>% 
  gt() %>% 
  fmt_number(
    columns = 'Number of IRA interactions',
    sep_mark = ",",
    decimals = F
  ) %>% 
  tab_header(
    title = "Accounts with which the IRA had heavy engagement"
  )

#gtsave(non_IRA_influential_accounts_table, 'IRA_accounts.png')

```

As all of these accounts are identifiable as not being run by the IRA 


This provides some diagnostic analysis of our model to inform our selection of the number of clusters. As these processes can be quite memory intensive (due to parallel processing creating more than one instance of R) I am going to remove some objects from my environment first. 

```{r}
rm(tweet_dates_graph) 
rm(competency_tweets)
rm(competency_tokens)
rm(path)
rm(retweet_network_graph)
rm(tweet_dates_graph)
rm(tweets)

### ok that should make sure that you / I don't run out of memory. 
```



```{r}
MH17_sparse <- influential_MH17_tokens %>% 
  count(rows, word) %>% 
  cast_sparse(rows, word, n)
```


```{r}
many_models <- tibble(K = c(20, 40, 50, 60, 75, 100)) %>%
  mutate(topic_model = future_map(K, ~stm::stm(MH17_sparse, K = .,  verbose = FALSE))) #future_map here is the multicore processing version of {purrr's} map(). Given there is quite a lot of data to be analysed this should speed it up a bit. 
```

```{r}
heldout <- make.heldout(MH17_sparse)

k_result <- many_models %>%
  mutate(exclusivity = map(topic_model, exclusivity),
         semantic_coherence = map(topic_model, semanticCoherence, MH17_sparse),
         eval_heldout = map(topic_model, eval.heldout, heldout$missing),
         residual = map(topic_model, checkResiduals, MH17_sparse),
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))


```


```{r fig.height=2}

k_result_image <- k_result %>%
  transmute(K,
            Exclusivity = map_dbl(exclusivity, mean),
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  filter(Metric %in% c('Residuals', 'Semantic coherence', 'Held-out likelihood', 'Exclusivity')) %>% 
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics",
       subtitle = "These diagnostics indicate that a good number of topics would be around 50")

k_result_image
```


